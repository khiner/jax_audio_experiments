{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "* ~Try a built-in optimizer (like grad descent w/ momentum) instead of constant learning rate. See how convergence time changes~\n",
    "* ~Plot loss~\n",
    "* ~Refactor (processors are modules)~\n",
    "* ~Define `init_params` methods for each processor, initializing to params that have no effect on the signal (no-op  prior)~\n",
    "  - this is something to note in writeup. talk about small changes to params having big effect in processors with feedback. And how starting from no effect initially is good for live performance settings.\n",
    "* ~Add a simple clipping nonlinearity effect to see how `grad` does with that~\n",
    "* ~Refactoring~\n",
    "  - ~Use dicts for labeled params instead of n-d arrays (jax handles this)~\n",
    "  - ~Move parameter label creation into processor fns~\n",
    "* ~Estimate params for multiple serially-connected filters (create a general `serial_processors` processor)~\n",
    "* ~~Run on GPU & measure performance differences~~\n",
    "  - ~~Performance is indeed much better on a GPU, despite the serial bottlneck of the IIR filters.~~\n",
    "  - ~~E.g. for a length-5 IIR filter with an input sequence of length 300, on a GPU:\n",
    "    `CPU times: user 14.6 s, sys: 664 ms, total: 15.3 s, Wall time: 15.7 s`.\n",
    "    On the CPU, it wouldn't even finish for more than 5 minutes so I stopped it and dropped the input length to 100, and got these numbers; `CPU times: user 37.1 s, sys: 816 ms, total: 38 s, Wall time: 37.7 s`~~\n",
    "* ~~Add `tick_buffer` methods to all processors, which will allow for fast convolution FIR implementations and use of `lax.scan`~~\n",
    "* ~~How to speed up IIR filters (like allpass filters)?~~ This is basically completely resolved with the current `lax.scan`-based implementation!\n",
    "* ~~Use minibatches instead of just a single unit impulse signal. See if it finds parameters closer to ground truth.~~\n",
    "  - Done - averaging gradients over minibatches doesn't seem to have a big effect on accuracy. But it's good to have this in place and using `vmap` to vectorize across multiple training pairs provides some speedup\n",
    "* ~~Try different optimizers, learning rates, gradient clipping, and any other techniques to guide to better parameter spaces for IIR filters (always wants to over-compensate with a large a[0] parameter to scale the full output)~~\n",
    "  - Eh, I tried a few optimizers, learning rates, tried weight norm clipping. I ultimately just stopped optimizing the output gain param in the IIR filter (`a[0]`) since it's not commonly used anyway.\n",
    "* Add a `DelayLine` processor. This is fundamentally a FIR filter, with the important difference that it's only parameterized by a couple params (like delay length and dry/wet level), rather than one param for each coefficient. The single-sample `tick` method can show off the ability to implement in the more traditional read/write pointer style as well.\n",
    "* Add `AllpassFilter` and `CombFilter` processors\n",
    "* Test on a realistic 4-second audio signal at 24kHz\n",
    "* Animate changes in output signal over time, compared with target\n",
    "* Use a perceptual loss function instead of mse (steal DDSP's multi-scale spectral loss fn)\n",
    "* Improve performance\n",
    "  - ~Speed things up with [JIT](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#Using-jit-to-speed-up-functions)~\n",
    "  - What if we estimate parameters without running across the full input sample? Like just test against a very small input sample? E.g. a length-5 IIR filter shouldn't need to backprop against every sample of a 2-second clip. That's just super redundant, right? Its behavior should be fully determinable from an input sample of a length on the order of the coefficients I think. This could generalize to other processors as well. (Maybe something like a `testLength` for each processor.) Should be able to test this well by comparing loss as `testLength` drops.\n",
    "* ~I don't think functions truly need to be pure. I tried passing in a plain np array and changing it in place and it seemed to work fine. Maybe we can use shared buffers to improve memory usage?~\n",
    "  - Answer - this is because you can create and use state _inside_ functions transformed by JAX. I found a decent middleground here that lets me use processor classes with internal state by instantiating them inside the transformed fn.\n",
    "* End-goal (ready for blog post): [Implement freeverb and perform dereverbing](https://trello.com/c/NSnb806w/2-goal-parameterize-freeverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "sys.path.append('./processors')\n",
    "import fir_filter, iir_filter, clip, serial_processors\n",
    "from plotters import plot_filter, plot_loss, plot_params\n",
    "from train import train, evaluate, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches=100\n",
    "batch_size=4\n",
    "n_train = 200\n",
    "samples_per_second = 44_100\n",
    "n_samples = 1 * samples_per_second\n",
    "Xs = np.random.randn(n_train, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_processors(processors, reference_fn=None, plot=False):\n",
    "    start = time.time()\n",
    "    params_estimated, params_target, params_history, loss_history = train(processors, Xs, num_batches=n_batches, batch_size=batch_size)\n",
    "    print('Train time: {:.3E} s'.format(time.time() - start))\n",
    "    X_eval = Xs[0]\n",
    "    Y_estimated, Y_target = evaluate(params_estimated, params_target, serial_processors, X_eval, processors)\n",
    "    Y_reference = reference_fn(X_eval, params_target) if reference_fn is not None else None\n",
    "\n",
    "    if plot:\n",
    "        title = ' + '.join(processor.NAME for processor in processors)\n",
    "        plot_loss(loss_history)\n",
    "        plot_params(params_target, params_history)\n",
    "        plot_filter(X_eval, Y_target, Y_reference, Y_estimated, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_processors([fir_filter], lambda X, params: signal.lfilter(params[fir_filter.NAME]['B'], [1.0], X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_processors([iir_filter], lambda X, params: signal.lfilter(params[iir_filter.NAME]['B'], params[iir_filter.NAME]['A'], X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_processors([clip], lambda X, params: np.clip(X, params[clip.NAME]['min'], params[clip.NAME]['max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_processors([iir_filter, clip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# from scipy.io.wavfile import read as wavread\n",
    "\n",
    "# wav_fs, audio = wavread('speech-male.wav')\n",
    "# audio = audio.astype(float) / audio.max()\n",
    "# print('Original audio:')\n",
    "# IPython.display.Audio(audio, rate=wav_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
