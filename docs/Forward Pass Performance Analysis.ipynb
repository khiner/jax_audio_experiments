{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from jaxdsp.processors import fir_filter, iir_filter, clip, delay_line, lowpass_feedback_comb_filter as lbcf, allpass_filter, freeverb, serial_processors\n",
    "from jaxdsp.training import train, evaluate, process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* rename git:khiner/jax_audio_experiments to git:khiner/python_audio_processors with a `/processors/jax` directory\n",
    "* copy all (non-jax) audio processors in git:khiner/jupyter_notebooks here\n",
    "* publish `python_audio_processors` as a public library\n",
    "* delete all (now duplicated) processors in git:khiner/jupyter_notebooks, replace with `python_audio_processors` imports\n",
    "* move all existing analysis into a new `/analysis` directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import read as readwav\n",
    "\n",
    "# sample_rate = 44_100\n",
    "# X = np.random.randn(sample_rate * 40)\n",
    "sample_rate, X = readwav('../audio/speech-male.wav')\n",
    "Audio(X, rate=sample_rate)\n",
    "tail_length = 24 * sample_rate # let it ring out\n",
    "X = np.concatenate([X, np.zeros(tail_length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = freeverb\n",
    "presets = processor.PRESETS\n",
    "audio_for_preset = {preset_name: processor.tick_buffer({'params': params, 'state': processor.init_state()}, X)[1] for preset_name, params in presets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid a harsh clip at the end of the sample.\n",
    "def apply_release(X, release_samples=int(0.2*sample_rate)):\n",
    "    return X * np.concatenate([np.ones(X.shape[-1] - release_samples), np.linspace(1.0, 0.0, release_samples)])\n",
    "\n",
    "output_for_preset = {preset_name: Audio(apply_release(audio.T), rate=sample_rate) for preset_name, audio in audio_for_preset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_for_preset['flat_space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_for_preset['expanding_space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* demonstrate single-sample tick\n",
    "* plot processing time normalized to real_time = 1.0\n",
    "* implement streaming processing (chunked)\n",
    "  - Okay, I think this is the path:\n",
    "    * Client web application uses Web Audio API to stream microphone to WebRTC peer ([example](https://webrtc.github.io/samples/src/content/peerconnection/webaudio-input/))\n",
    "    * Set up a locally-running Flask server listening to a WebRTC stream ([example](https://github.com/aiortc/aiortc/blob/main/examples/server/server.py))\n",
    "    * Process the incoming audio chunks, then send them back to the stream\n",
    "  - Example matplotlib charts with streaming audio from microphone: https://github.com/markjay4k/Audio-Spectrum-Analyzer-in-Python\n",
    "  - This paper does what I'm trying to do: https://www.researchgate.net/publication/326885247_Real-Time_Digital_Signal_Processing_Using_pyaudio_helper_and_the_ipywidgets\n",
    "  - And this is what's behind that paper: https://github.com/mwickert/scikit-dsp-comm\n",
    "  - I think the above is too heavy-weight in terms of dependencies, and I'm working through broken deps.\n",
    "  - https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/webrtc-integration.html\n",
    "    * Keywords: WebRTC integration with the Web Audio API\n",
    "* compare to C++ performance\n",
    "* provide real-time control over parameters via js\n",
    "  - just in jupyter for now\n",
    "* perform inference real-time, displaying estimated parameter values via auto-controlled, disabled knobs\n",
    "  - interface idea: two separate reverb controllers, on top and bottom of screen, each with a knob for each param. You can mute or solo either reverb separately. At any point, you can hit the 'learn' button for either reverb. This will disable its knobs, and will then listening to the input & output audio of the other (still user-controllable) reverb and live-update its own parameters to try and match the other reverb parameter values.\n",
    "* check if URL is available: python_audio_processors.io\n",
    "  - otherwise, just use karlhiner.com/python_audio_processors\n",
    "* port to ^\n",
    "* charts for impulse response, magnitude spectrogram and phase, updating in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
